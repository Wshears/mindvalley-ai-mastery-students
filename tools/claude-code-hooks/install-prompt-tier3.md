# Install Prompt: Tier 3 - Contextual + Local LLM (FREE, Full Experience)

Copy and paste this entire prompt into Claude Code:

---

```
Help me set up the FULL completion notification experience - contextual messages generated by a local LLM, spoken with natural TTS. All free, all local.

## What I Want
When Claude Code finishes a task, I want to hear contextual notifications like:
- "Finished refactoring the authentication module"
- "Your pull request description is ready"
- "Successfully updated three configuration files"

Not just "done" - I want the notification to understand WHAT was done.

## Requirements
- 100% local, no API costs ever
- Contextual messages (not just templates)
- Natural-sounding voice
- Should work in <2 seconds total

## My Operating System
[Tell Claude: Mac / Windows / Linux]

## Please Do These Steps

### Part 1: Install Ollama (Local LLM)

1. Install Ollama:
   - Mac/Linux: curl -fsSL https://ollama.com/install.sh | sh
   - Windows: Download from ollama.com

2. Pull the fast model:
   ollama pull qwen2:0.5b

3. Test Ollama works:
   ollama run qwen2:0.5b "Say in 5 words: finished editing config file"

### Part 2: Install the Hook

4. Create hooks directory:
   mkdir -p ~/.claude/hooks

5. Install Python dependencies:
   pip install piper-tts requests

6. Copy hook-contextual-local.py from this repo's tools/claude-code-hooks/ folder

7. Save to ~/.claude/hooks/completion-notification.py

8. Add to ~/.claude/settings.json:
   ```json
   {
     "hooks": {
       "Stop": [
         {
           "hooks": [
             {
               "type": "command",
               "command": "python3 ~/.claude/hooks/completion-notification.py --hook Stop"
             }
           ]
         }
       ],
       "Notification": [
         {
           "hooks": [
             {
               "type": "command",
               "command": "python3 ~/.claude/hooks/completion-notification.py --hook Notification"
             }
           ]
         }
       ]
     }
   }
   ```

### Part 3: Test Everything

9. Test the full chain:
   echo '{"tool": "Edit", "file_path": "/test/auth.py"}' | python3 ~/.claude/hooks/completion-notification.py --hook Stop

10. Verify:
    - I hear a natural voice
    - Message is contextual (mentions the file)
    - Total time is <2 seconds

Walk me through each step, verify each works before moving on.
```

---

## What You'll Get

**Contextual messages like:**
- "Completed editing the authentication handler"
- "Finished updating the configuration"
- "Successfully ran the test suite"

**Not just:**
- "Done"
- "Task complete"

## How It Works

```
Hook triggered → Ollama generates contextual message → Piper speaks it
    ↓                     ↓                              ↓
  ~0ms              0.3-0.5s (LLM)                  0.2-0.3s (TTS)

Total: 500-1000ms
```

## Configuration

```bash
# Use a different LLM model (larger = better but slower)
export CLAUDE_HOOK_LLM="phi3.5"  # More natural language

# Change voice
export CLAUDE_HOOK_VOICE="en_GB-alan-medium"  # British accent

# Adjust timeout
export CLAUDE_HOOK_TIMEOUT="2.0"  # seconds
```

## Fallback Behavior

The hook gracefully handles failures:

1. **Ollama not running** → Uses template messages (still speaks)
2. **Ollama too slow** → Times out, uses template
3. **Piper not installed** → Falls back to system TTS
4. **Everything fails** → Desktop notification + console output

## Resource Usage

- **Ollama daemon**: ~200MB RAM (idle)
- **qwen2:0.5b loaded**: ~1.2GB RAM
- **Piper model**: ~50MB disk
- **CPU usage**: Brief spike during generation

## Troubleshooting

**Ollama not responding**
```bash
# Check if running
curl http://localhost:11434/api/tags

# Start if not running
ollama serve
```

**Messages are generic (not contextual)**
- Check Ollama is running: `ollama list`
- Check model is pulled: `ollama pull qwen2:0.5b`
- Check hook can reach Ollama: test with curl

**Voice sounds robotic**
- Piper not installed/working, falling back to system TTS
- Reinstall: `pip install piper-tts`

**First run is very slow**
- Normal - downloading voice model (~10-50MB)
- Subsequent runs are fast

**Want faster responses?**
- Message generation: Use smaller model or skip LLM (fall back to Tier 1/2)
- Voice generation: Already optimized with Piper
